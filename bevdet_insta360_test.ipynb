{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1aafcf0-dc26-461a-b2b8-1e348dd3845b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import mmcv\n",
    "import torch\n",
    "from mmcv import Config, DictAction\n",
    "from mmcv.cnn import fuse_conv_bn\n",
    "from mmcv.parallel import MMDataParallel, MMDistributedDataParallel\n",
    "from mmcv.runner import (get_dist_info, init_dist, load_checkpoint,wrap_fp16_model)\n",
    "\n",
    "import mmdet\n",
    "from mmdet3d.apis import single_gpu_test\n",
    "from mmdet3d.datasets import build_dataloader, build_dataset\n",
    "from mmdet3d.models import build_model\n",
    "from mmdet.apis import multi_gpu_test, set_random_seed\n",
    "from mmdet.datasets import replace_ImageToTensor\n",
    "\n",
    "if mmdet.__version__ > '2.23.0':\n",
    "    # If mmdet version > 2.23.0, setup_multi_processes would be imported and\n",
    "    # used from mmdet instead of mmdet3d.\n",
    "    from mmdet.utils import setup_multi_processes\n",
    "else:\n",
    "    from mmdet3d.utils import setup_multi_processes\n",
    "\n",
    "try:\n",
    "    # If mmdet version > 2.23.0, compat_cfg would be imported and\n",
    "    # used from mmdet instead of mmdet3d.\n",
    "    from mmdet.utils import compat_cfg\n",
    "except ImportError:\n",
    "    from mmdet3d.utils import compat_cfg\n",
    "    \n",
    "import json\n",
    "import pickle\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pyquaternion.quaternion import Quaternion\n",
    "from mmdet3d.core.bbox.structures.lidar_box3d import LiDARInstance3DBoxes as LB\n",
    "\n",
    "from nuscenes.utils.data_classes import Box, LidarPointCloud\n",
    "from nuscenes.utils.geometry_utils import view_points, box_in_image, BoxVisibility, transform_matrix\n",
    "from PIL import Image\n",
    "from pyquaternion import Quaternion\n",
    "import pyquaternion\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import glob\n",
    "from math import pi\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import mean_squared_error\n",
    "\n",
    "# PoseNet\n",
    "import torchvision.models as models\n",
    "from torch import nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68afc021-0571-4bbd-890f-75f7b477d96a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Pose Network (from Manydepth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5c3bb19-4227-488d-bcce-f5dbcb0bb029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation_from_parameters(axisangle, translation, invert=False):\n",
    "    \"\"\"Convert the network's (axisangle, translation) output into a 4x4 matrix\n",
    "    \"\"\"\n",
    "    R = rot_from_axisangle(axisangle)\n",
    "    t = translation.clone()\n",
    "\n",
    "    if invert:\n",
    "        R = R.transpose(1, 2)\n",
    "        t *= -1\n",
    "\n",
    "    T = get_translation_matrix(t)\n",
    "    \n",
    "    # print(\"R: \", R)\n",
    "    # print(\"R[:3, :3]: \", R[:, :3, :3])\n",
    "    # print(\"T: \", T)\n",
    "    # print(\"T[~]: \", T[:, :3, -1])\n",
    "    \n",
    "    # R, T를 이어 붙여서 return\n",
    "    M = R.new_zeros((4, 4))\n",
    "    M[3, 3] = 1\n",
    "    M[:3, :3] = R[:, :3, :3]\n",
    "    M[:3, -1] = T[:, :3, -1]\n",
    "\n",
    "    # R, T 간의 행렬곱을 return\n",
    "    # if invert:\n",
    "    #     M = torch.matmul(R, T)\n",
    "    # else:\n",
    "    #     M = torch.matmul(T, R)\n",
    "\n",
    "    return M\n",
    "\n",
    "def get_translation_matrix(translation_vector):\n",
    "    \"\"\"Convert a translation vector into a 4x4 transformation matrix\n",
    "    \"\"\"\n",
    "    T = torch.zeros(translation_vector.shape[0], 4, 4).to(device=translation_vector.device)\n",
    "\n",
    "    t = translation_vector.contiguous().view(-1, 3, 1)\n",
    "\n",
    "    T[:, 0, 0] = 1\n",
    "    T[:, 1, 1] = 1\n",
    "    T[:, 2, 2] = 1\n",
    "    T[:, 3, 3] = 1\n",
    "    T[:, :3, 3, None] = t\n",
    "\n",
    "    return T\n",
    "\n",
    "def rot_from_axisangle(vec):\n",
    "    \"\"\"Convert an axisangle rotation into a 4x4 transformation matrix\n",
    "    (adapted from https://github.com/Wallacoloo/printipi)\n",
    "    Input 'vec' has to be Bx1x3\n",
    "    \"\"\"\n",
    "    angle = torch.norm(vec, 2, 2, True)\n",
    "    axis = vec / (angle + 1e-7)\n",
    "\n",
    "    ca = torch.cos(angle)\n",
    "    sa = torch.sin(angle)\n",
    "    C = 1 - ca\n",
    "\n",
    "    x = axis[..., 0].unsqueeze(1)\n",
    "    y = axis[..., 1].unsqueeze(1)\n",
    "    z = axis[..., 2].unsqueeze(1)\n",
    "\n",
    "    xs = x * sa\n",
    "    ys = y * sa\n",
    "    zs = z * sa\n",
    "    xC = x * C\n",
    "    yC = y * C\n",
    "    zC = z * C\n",
    "    xyC = x * yC\n",
    "    yzC = y * zC\n",
    "    zxC = z * xC\n",
    "\n",
    "    rot = torch.zeros((vec.shape[0], 4, 4)).to(device=vec.device)\n",
    "\n",
    "    rot[:, 0, 0] = torch.squeeze(x * xC + ca)\n",
    "    rot[:, 0, 1] = torch.squeeze(xyC - zs)\n",
    "    rot[:, 0, 2] = torch.squeeze(zxC + ys)\n",
    "    rot[:, 1, 0] = torch.squeeze(xyC + zs)\n",
    "    rot[:, 1, 1] = torch.squeeze(y * yC + ca)\n",
    "    rot[:, 1, 2] = torch.squeeze(yzC - xs)\n",
    "    rot[:, 2, 0] = torch.squeeze(zxC - ys)\n",
    "    rot[:, 2, 1] = torch.squeeze(yzC + xs)\n",
    "    rot[:, 2, 2] = torch.squeeze(z * zC + ca)\n",
    "    rot[:, 3, 3] = 1\n",
    "\n",
    "    return rot\n",
    "\n",
    "class ResNetMultiImageInput(models.ResNet):\n",
    "    \"\"\"Constructs a resnet model with varying number of input images.\n",
    "    Adapted from https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, num_input_images=1):\n",
    "        super(ResNetMultiImageInput, self).__init__(block, layers)\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            num_input_images * 3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "def resnet_multiimage_input(num_layers, pretrained=False, num_input_images=1):\n",
    "    \"\"\"Constructs a ResNet model.\n",
    "    Args:\n",
    "        num_layers (int): Number of resnet layers. Must be 18 or 50\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        num_input_images (int): Number of frames stacked as input\n",
    "    \"\"\"\n",
    "    assert num_layers in [18, 50], \"Can only run with 18 or 50 layer resnet\"\n",
    "    blocks = {18: [2, 2, 2, 2], 50: [3, 4, 6, 3]}[num_layers]\n",
    "    block_type = {18: models.resnet.BasicBlock, 50: models.resnet.Bottleneck}[num_layers]\n",
    "    model = ResNetMultiImageInput(block_type, blocks, num_input_images=num_input_images)\n",
    "\n",
    "    if pretrained:\n",
    "        loaded = model_zoo.load_url(models.resnet.model_urls['resnet{}'.format(num_layers)])\n",
    "        loaded['conv1.weight'] = torch.cat(\n",
    "            [loaded['conv1.weight']] * num_input_images, 1) / num_input_images\n",
    "        model.load_state_dict(loaded)\n",
    "    return model\n",
    "\n",
    "\"\"\"Pose Encoder\"\"\"\n",
    "class ResnetEncoder(nn.Module):\n",
    "    \"\"\"Pytorch module for a resnet encoder\"\"\"\n",
    "    def __init__(self, num_layers, pretrained, num_input_images=1, **kwargs):\n",
    "        super(ResnetEncoder, self).__init__()\n",
    "\n",
    "        self.num_ch_enc = np.array([64, 64, 128, 256, 512])\n",
    "\n",
    "        resnets = {18: models.resnet18,\n",
    "                   34: models.resnet34,\n",
    "                   50: models.resnet50,\n",
    "                   101: models.resnet101,\n",
    "                   152: models.resnet152}\n",
    "\n",
    "        if num_layers not in resnets:\n",
    "            raise ValueError(\"{} is not a valid number of resnet layers\".format(num_layers))\n",
    "\n",
    "        if num_input_images > 1:\n",
    "            self.encoder = resnet_multiimage_input(num_layers, pretrained, num_input_images)\n",
    "        else:\n",
    "            self.encoder = resnets[num_layers](pretrained)\n",
    "\n",
    "        if num_layers > 34:\n",
    "            self.num_ch_enc[1:] *= 4\n",
    "\n",
    "    def forward(self, input_image):\n",
    "        self.features = []\n",
    "        x = (input_image - 0.45) / 0.225\n",
    "        x = self.encoder.conv1(x)\n",
    "        x = self.encoder.bn1(x)\n",
    "        self.features.append(self.encoder.relu(x))\n",
    "        self.features.append(self.encoder.layer1(self.encoder.maxpool(self.features[-1])))\n",
    "        self.features.append(self.encoder.layer2(self.features[-1]))\n",
    "        self.features.append(self.encoder.layer3(self.features[-1]))\n",
    "        self.features.append(self.encoder.layer4(self.features[-1]))\n",
    "\n",
    "        return self.features\n",
    "\n",
    "\"\"\"Pose Decoder\"\"\"\n",
    "class PoseDecoder(nn.Module):\n",
    "    def __init__(self, num_ch_enc, num_input_features, num_frames_to_predict_for=None, stride=1):\n",
    "        super(PoseDecoder, self).__init__()\n",
    "\n",
    "        self.num_ch_enc = num_ch_enc\n",
    "        self.num_input_features = num_input_features\n",
    "\n",
    "        if num_frames_to_predict_for is None:\n",
    "            num_frames_to_predict_for = num_input_features - 1\n",
    "        self.num_frames_to_predict_for = num_frames_to_predict_for\n",
    "\n",
    "        self.convs = OrderedDict()\n",
    "        self.convs[(\"squeeze\")] = nn.Conv2d(self.num_ch_enc[-1], 256, 1)\n",
    "        self.convs[(\"pose\", 0)] = nn.Conv2d(num_input_features * 256, 256, 3, stride, 1)\n",
    "        self.convs[(\"pose\", 1)] = nn.Conv2d(256, 256, 3, stride, 1)\n",
    "        self.convs[(\"pose\", 2)] = nn.Conv2d(256, 6 * num_frames_to_predict_for, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.net = nn.ModuleList(list(self.convs.values()))\n",
    "\n",
    "    def forward(self, input_features):\n",
    "        last_features = [f[-1] for f in input_features]\n",
    "\n",
    "        cat_features = [self.relu(self.convs[\"squeeze\"](f)) for f in last_features]\n",
    "        cat_features = torch.cat(cat_features, 1)\n",
    "\n",
    "        out = cat_features\n",
    "        for i in range(3):\n",
    "            out = self.convs[(\"pose\", i)](out)\n",
    "            if i != 2:\n",
    "                out = self.relu(out)\n",
    "\n",
    "        out = out.mean(3).mean(2)\n",
    "\n",
    "        out = 0.01 * out.view(-1, self.num_frames_to_predict_for, 1, 6)\n",
    "\n",
    "        axisangle = out[..., :3]\n",
    "        translation = out[..., 3:]\n",
    "\n",
    "        return axisangle, translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db8effe-3b9c-45d9-83a8-66eba8a06ac6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Tangent Patch Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd805d25-0583-4743-a853-35a7f9dd7a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createProjectGrid(erp_h, erp_w, tangent_h, tangent_w, num_rows, num_cols, phi_centers, fov):\n",
    "    height, width = tangent_h, tangent_w\n",
    "\n",
    "    FOV = fov\n",
    "    FOV = [FOV[0] / 360.0, FOV[1] / 180.0]\n",
    "    FOV = torch.tensor(FOV, dtype=torch.float32)\n",
    "\n",
    "    PI = math.pi\n",
    "    PI_2 = math.pi * 0.5\n",
    "    PI2 = math.pi * 2\n",
    "\n",
    "    yy, xx = torch.meshgrid(torch.linspace(0, 1, height), torch.linspace(0, 1, width))\n",
    "    screen_points = torch.stack([xx.flatten(), yy.flatten()], -1)\n",
    "\n",
    "    num_rows = num_rows\n",
    "    num_cols = num_cols\n",
    "    phi_centers = phi_centers\n",
    "\n",
    "    phi_interval = 180 // num_rows\n",
    "    all_combos = []\n",
    "    erp_mask = []\n",
    "\n",
    "    for i, n_cols in enumerate(num_cols):\n",
    "        for j in np.arange(n_cols): # 0 ~ num_cols.length\n",
    "            theta_interval = 360 / n_cols # 현재 row (위도)에서 쪼개질 경도 (col)의 위치\n",
    "            theta_center = j * theta_interval + theta_interval / 2\n",
    "            center = [theta_center, phi_centers[i]] # 각 tangent image의 center position\n",
    "\n",
    "            # print(str(j) + \" th theta center \" + str(theta_center) + \" phi center \" + str(phi_centers[i]))\n",
    "            \n",
    "            all_combos.append(center)\n",
    "\n",
    "            # 구좌표계에서의 tangent image가 차지하는 영역에 대한 좌표들\n",
    "            up = phi_centers[i] + phi_interval / 2\n",
    "            down = phi_centers[i] - phi_interval / 2\n",
    "            left = theta_center - theta_interval / 2\n",
    "            right = theta_center + theta_interval / 2\n",
    "\n",
    "            # ERP image에서 현재 tangent가 차지하는 영역에 대한 pixel 위치들\n",
    "            up = int((up + 90) / 180 * erp_h)\n",
    "            down = int((down + 90) / 180 * erp_h)\n",
    "            left = int(left / 360 * erp_w)\n",
    "            right = int(right / 360 * erp_w)\n",
    "\n",
    "            # ERP 이미지에서 현재 tangent image 영역에 해당하는 부분에 1로 마스킹\n",
    "            mask = np.zeros((erp_h, erp_w), dtype=int)\n",
    "            mask[down:up, left:right] = 1\n",
    "            erp_mask.append(mask)\n",
    "\n",
    "    all_combos = np.vstack(all_combos)\n",
    "    shifts = np.arange(all_combos.shape[0]) * width\n",
    "    shifts = torch.from_numpy(shifts).float()\n",
    "    erp_mask = np.stack(erp_mask)\n",
    "    erp_mask = torch.from_numpy(erp_mask).float()\n",
    "    n_patch = all_combos.shape[0]\n",
    "    \n",
    "    center_point = torch.from_numpy(all_combos).float()  # -180 to 180, -90 to 90\n",
    "    center_point[:, 0] = (center_point[:, 0]) / 360  #0 to 1\n",
    "    center_point[:, 1] = (center_point[:, 1] + 90) / 180  #0 to 1\n",
    "\n",
    "    cp = center_point * 2 - 1\n",
    "    cp[:, 0] = cp[:, 0] * PI\n",
    "    cp[:, 1] = cp[:, 1] * PI_2\n",
    "    cp = cp.unsqueeze(1)\n",
    "\n",
    "    convertedCoord = screen_points * 2 - 1\n",
    "    convertedCoord[:, 0] = convertedCoord[:, 0] * PI\n",
    "    convertedCoord[:, 1] = convertedCoord[:, 1] * PI_2\n",
    "    convertedCoord = convertedCoord * (torch.ones(screen_points.shape, dtype=torch.float32) * FOV)\n",
    "    convertedCoord = convertedCoord.unsqueeze(0).repeat(cp.shape[0], 1, 1)\n",
    "\n",
    "    x = convertedCoord[:, :, 0]\n",
    "    y = convertedCoord[:, :, 1]\n",
    "\n",
    "    rou = torch.sqrt(x ** 2 + y ** 2)\n",
    "    c = torch.atan(rou)\n",
    "    sin_c = torch.sin(c)\n",
    "    cos_c = torch.cos(c)\n",
    "    lat = torch.asin(cos_c * torch.sin(cp[:, :, 1]) + (y * sin_c * torch.cos(cp[:, :, 1])) / rou)\n",
    "    lon = cp[:, :, 0] + torch.atan2(x * sin_c, rou * torch.cos(cp[:, :, 1]) * cos_c - y * torch.sin(cp[:, :, 1]) * sin_c)\n",
    "    lat_new = lat / PI_2\n",
    "    lon_new = lon / PI\n",
    "    lon_new[lon_new > 1] -= 2\n",
    "    lon_new[lon_new<-1] += 2\n",
    "\n",
    "    lon_new = lon_new.view(1, n_patch, height, width).permute(0, 2, 1, 3).contiguous().view(height, n_patch*width)\n",
    "    lat_new = lat_new.view(1, n_patch, height, width).permute(0, 2, 1, 3).contiguous().view(height, n_patch*width)\n",
    "    \n",
    "    grid = torch.stack([lon_new, lat_new], -1)\n",
    "    grid = grid.unsqueeze(0)\n",
    "\n",
    "    return n_patch, grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a6c7ba-fe9a-4973-8223-4afa6279db1a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21718df3-4885-4051-8dcd-37b8618bb6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quaternion_from_euler(e):\n",
    "    \"\"\"\n",
    "    Convert an Euler angle to a quaternion.\n",
    "\n",
    "    Input\n",
    "    :param roll: The roll (rotation around x-axis) angle in radians.\n",
    "    :param pitch: The pitch (rotation around y-axis) angle in radians.\n",
    "    :param yaw: The yaw (rotation around z-axis) angle in radians.\n",
    "\n",
    "    Output\n",
    "    :return qx, qy, qz, qw: The orientation in quaternion [x,y,z,w] format\n",
    "    \"\"\"\n",
    "    roll = e[0]\n",
    "    pitch = e[1]\n",
    "    yaw = e[2]\n",
    "\n",
    "    qx = np.sin(roll/2) * np.cos(pitch/2) * np.cos(yaw/2) - np.cos(roll/2) * np.sin(pitch/2) * np.sin(yaw/2)\n",
    "    qy = np.cos(roll/2) * np.sin(pitch/2) * np.cos(yaw/2) + np.sin(roll/2) * np.cos(pitch/2) * np.sin(yaw/2)\n",
    "    qz = np.cos(roll/2) * np.cos(pitch/2) * np.sin(yaw/2) - np.sin(roll/2) * np.sin(pitch/2) * np.cos(yaw/2)\n",
    "    qw = np.cos(roll/2) * np.cos(pitch/2) * np.cos(yaw/2) + np.sin(roll/2) * np.sin(pitch/2) * np.sin(yaw/2)\n",
    "\n",
    "    return [qw, qx, qy, qz]\n",
    "\n",
    "def euler_from_quaternion(q):\n",
    "    \"\"\"\n",
    "    Convert a quaternion into euler angles (roll, pitch, yaw)\n",
    "    roll is rotation around x in radians (counterclockwise)\n",
    "    pitch is rotation around y in radians (counterclockwise)\n",
    "    yaw is rotation around z in radians (counterclockwise)\n",
    "    \"\"\"\n",
    "    import math\n",
    "    w = q[0]\n",
    "    x = q[1]\n",
    "    y = q[2]\n",
    "    z = q[3]\n",
    "    \n",
    "    t0 = +2.0 * (w * x + y * z)\n",
    "    t1 = +1.0 - 2.0 * (x * x + y * y)\n",
    "    # roll_x = math.atan2(t0, t1) / np.pi * 180 # degrees\n",
    "    roll_x = math.atan2(t0, t1)\n",
    "\n",
    "    t2 = +2.0 * (w * y - z * x)\n",
    "    t2 = +1.0 if t2 > +1.0 else t2\n",
    "    t2 = -1.0 if t2 < -1.0 else t2\n",
    "    # pitch_y = math.asin(t2) / np.pi * 180\n",
    "    pitch_y = math.asin(t2)\n",
    "\n",
    "    t3 = +2.0 * (w * z + x * y)\n",
    "    t4 = +1.0 - 2.0 * (y * y + z * z) \n",
    "    # yaw_z = math.atan2(t3, t4) / np.pi * 180\n",
    "    yaw_z = math.atan2(t3, t4)\n",
    "\n",
    "    return [roll_x, pitch_y, yaw_z] # in radian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffe2d07-d70f-410b-9e90-7211fa76a31c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1322eb-7491-4fce-9455-7a53e5534a9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Init Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2529dde-b95a-4c2a-9759-6a94897cf841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/jeholee/anaconda3/envs/omnicv/lib/python3.8/site-packages/mmdet/models/backbones/resnet.py:401: UserWarning: DeprecationWarning: pretrained is deprecated, please use \"init_cfg\" instead\n",
      "  warnings.warn('DeprecationWarning: pretrained is deprecated, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: ./ckpt/bevdet4d-r50-depth-cbgs.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/jeholee/anaconda3/envs/omnicv/lib/python3.8/site-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\"\n",
    "\n",
    "config = \"./configs/bevdet/bevdet4d-r50-depth-cbgs.py\"\n",
    "ckpt = \"./ckpt/bevdet4d-r50-depth-cbgs.pth\"\n",
    "\n",
    "# config = \"./configs/bevdet/bevdet-r50-cbgs.py\"\n",
    "# ckpt = \"./ckpt/bevdet-r50-cbgs.pth\"\n",
    "\n",
    "\"\"\" Init configuration \"\"\"\n",
    "cfg = Config.fromfile(config)\n",
    "cfg = compat_cfg(cfg)\n",
    "\n",
    "if cfg.get('cudnn_benchmark', False): # set cudnn_benchmark\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\"\"\" Init BEVDet model \"\"\"\n",
    "cfg.model.pretrained = None\n",
    "distributed = False\n",
    "if '4D' in cfg.model.type: # video-based or not\n",
    "    cfg.model.align_after_view_transfromation=True\n",
    "cfg.model.train_cfg = None\n",
    "\n",
    "model = build_model(cfg.model, test_cfg=cfg.get('test_cfg'))\n",
    "\n",
    "fp16_cfg = cfg.get('fp16', None)\n",
    "if fp16_cfg is not None:\n",
    "    wrap_fp16_model(model)\n",
    "checkpoint = load_checkpoint(model, ckpt, map_location=device) # or CPU?\n",
    "model.CLASSES = cfg.class_names\n",
    "\n",
    "\"\"\" Init PoseNet \"\"\"\n",
    "manydepth_model_path = './manydepth/ckpt/KITTI_MR/'\n",
    "\n",
    "pose_enc_dict = torch.load(os.path.join(manydepth_model_path, \"pose_encoder.pth\"), map_location=device)\n",
    "pose_dec_dict = torch.load(os.path.join(manydepth_model_path, \"pose.pth\"), map_location=device)\n",
    "\n",
    "pose_enc = ResnetEncoder(18, False, num_input_images=2)\n",
    "pose_dec = PoseDecoder(pose_enc.num_ch_enc, num_input_features=1, num_frames_to_predict_for=2)\n",
    "\n",
    "pose_enc.load_state_dict(pose_enc_dict, strict=True)\n",
    "pose_dec.load_state_dict(pose_dec_dict, strict=True)\n",
    "\n",
    "\"\"\" Models to GPU memory \"\"\"\n",
    "model.eval()\n",
    "model.to(device)\n",
    "pose_enc.eval()\n",
    "pose_dec.eval()\n",
    "pose_enc.to(device)\n",
    "pose_dec.to(device)\n",
    "\n",
    "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "\"\"\" Init Tangent Projection Grid \"\"\"\n",
    "num_rows = 1\n",
    "num_cols = [6]\n",
    "phi_centers = [0]\n",
    "\n",
    "# fov 가로/세로 비율과 nuscenes input의 width/height 비율이 같음\n",
    "# 900/1600, 396/704: 1.777777...\n",
    "# 위 비율에 맞춰서 tangent patch size 결정하기\n",
    "# 중요! 704, 256은 aspect ratio가 다름. 원래 코드에서도 704, 396으로 resize하고 추후에 704, 256으로 crop함\n",
    "tangent_h = 396 # 256 # 900 # 396\n",
    "tangent_w = 704 # 704 # 1600 # 704\n",
    "fov  = [70, 39.375]\n",
    "erp_h, erp_w = 1920, 3840\n",
    "\n",
    "n_patch, grid = createProjectGrid(erp_h, erp_w, tangent_h, tangent_w, num_rows, num_cols, phi_centers, fov)\n",
    "grid = grid.to(device)\n",
    "\n",
    "vis_tangent_h = 900 # visualization resolution\n",
    "vis_tangent_w = 1600\n",
    "\n",
    "n_patch, vis_grid = createProjectGrid(erp_h, erp_w, vis_tangent_h, vis_tangent_w, num_rows, num_cols, phi_centers, fov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1d5b7d-bf36-4a63-a791-7fafdb202395",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Init Insta360 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "496fbda8-96b9-4bc2-95e6-6bed358d371d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/home/jeholee/omni3D/data/nuscenes/\n",
      "(1, 9, 1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Init dataset, data loader, infos \"\"\"\n",
    "cfg.data.test.test_mode = True\n",
    "if cfg.data.test_dataloader.get('samples_per_gpu', 1) > 1:\n",
    "    # Replace 'ImageToTensor' to 'DefaultFormatBundle'\n",
    "    cfg.data.test.pipeline = replace_ImageToTensor(cfg.data.test.pipeline)\n",
    "\n",
    "test_dataloader_default_args = dict(samples_per_gpu=1, workers_per_gpu=1, dist=distributed, shuffle=False)\n",
    "test_loader_cfg = {\n",
    "        **test_dataloader_default_args,\n",
    "        **cfg.data.get('test_dataloader', {})\n",
    "    }\n",
    "\n",
    "dataset = build_dataset(cfg.data.test)\n",
    "data_loader = build_dataloader(dataset, **test_loader_cfg)\n",
    "\n",
    "infos = dataset.data_infos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c138a03-f92e-4798-9c29-b902147bbd01",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/jeholee/omni3D/BEVDet/mmdet3d/datasets/pipelines/loading.py:347: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755903507/work/torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  gt_boxes, gt_labels = torch.Tensor(gt_boxes), torch.tensor(gt_labels)\n",
      "/data/home/jeholee/omni3D/BEVDet/mmdet3d/datasets/pipelines/loading.py:347: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755903507/work/torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  gt_boxes, gt_labels = torch.Tensor(gt_boxes), torch.tensor(gt_labels)\n",
      "/data/home/jeholee/omni3D/BEVDet/mmdet3d/datasets/pipelines/loading.py:347: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755903507/work/torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  gt_boxes, gt_labels = torch.Tensor(gt_boxes), torch.tensor(gt_labels)\n",
      "/data/home/jeholee/omni3D/BEVDet/mmdet3d/datasets/pipelines/loading.py:347: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755903507/work/torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  gt_boxes, gt_labels = torch.Tensor(gt_boxes), torch.tensor(gt_labels)\n"
     ]
    }
   ],
   "source": [
    "data_iterator = iter(data_loader)\n",
    "data = next(data_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e783865-5a9d-40d4-98db-c46e813a562e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample_idx = 8\n",
    "# data_iterator = iter(data_loader)\n",
    "\n",
    "# cnt = -1\n",
    "# while cnt < sample_idx:\n",
    "#     data = next(data_iterator)\n",
    "#     cnt += 1\n",
    "# print(sample_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2201118-495c-4fdb-bf7d-a71b1d1ab36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_dir = \"scene_3/\"\n",
    "erp_img_root = \"../data/daejeon_road_outdoor/erp_images/\"\n",
    "\n",
    "erp_files = []\n",
    "for filename in os.listdir(erp_img_root + scene_dir):\n",
    "    erp_files.append(os.path.join(erp_img_root, scene_dir, filename))\n",
    "\n",
    "def get_erp_img(fname):\n",
    "    # load images\n",
    "    print(f\"Load erp image: {fname}\")\n",
    "    erp_img = cv2.imread(fname, cv2.IMREAD_COLOR)\n",
    "    erp_img = erp_img.astype(np.float32) / 255\n",
    "    erp_img = np.transpose(erp_img, [2, 0, 1]) # permutation, 세 번째 axis가 첫 번째 axis로\n",
    "    erp_img = torch.from_numpy(erp_img) # Create Tensor from numpy array\n",
    "    erp_img = erp_img.unsqueeze(0) # Increase Tensor dimension by 1\n",
    "    return erp_img\n",
    "\n",
    "img_conf = dict(img_mean=[123.675, 116.28, 103.53],\n",
    "                img_std=[58.395, 57.12, 57.375],\n",
    "                to_rgb=True)\n",
    "\n",
    "H, W = cfg.data_config['src_size']\n",
    "fH, fW = cfg.data_config['input_size']\n",
    "newH, newW = tangent_h, tangent_w\n",
    "crop_h = int((1 - np.mean(cfg.data_config['crop_h'])) * newH) - fH\n",
    "crop_w = int(max(0, newW - fW) / 2)\n",
    "crop = (crop_w, crop_h, crop_w + fW, crop_h + fH)\n",
    "\n",
    "# post-homography transformation\n",
    "post_rot = torch.eye(2)\n",
    "post_tran = torch.zeros(2)\n",
    "\n",
    "resize = float(fW) / float(W)\n",
    "resize += cfg.data_config.get('resize_test', 0.0)\n",
    "rotate = 0\n",
    "\n",
    "post_rot *= resize\n",
    "post_tran -= torch.Tensor(crop[:2])\n",
    "\n",
    "rot_h = rotate / 180 * np.pi\n",
    "A = torch.Tensor([[np.cos(rot_h), np.sin(rot_h)], [-np.sin(rot_h), np.cos(rot_h)]])\n",
    "b = torch.Tensor([crop[2] - crop[0], crop[3] - crop[1]]) / 2\n",
    "b = A.matmul(-b) + b\n",
    "\n",
    "post_rot2 = A.matmul(post_rot)\n",
    "post_tran2 = A.matmul(post_tran) + b\n",
    "post_tran = torch.zeros(3) # for convenience, make augmentation matrices 3x3\n",
    "post_rot = torch.eye(3)\n",
    "post_tran[:2] = post_tran2\n",
    "post_rot[:2, :2] = post_rot2\n",
    "\n",
    "campose_input_crop = (0, 168, 640, 360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05232422-c9b5-459c-a03e-c66231368228",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Collect current and adjacent erp images\n",
    "# \"\"\" erp_imgs: [0 (cur), 1...8 (prev)] \"\"\"\n",
    "# adj_erp_imgs = []\n",
    "# print(\"current\")\n",
    "# cur_erp_img = get_erp_img(erp_files[sample_idx]) # current erp image\n",
    "\n",
    "# print(\"adjacent\")\n",
    "# for select_id in range(*cfg.multi_adj_frame_id_cfg): # sample_idx의 이전 8 (개수는 변경 가능) frames를 얻어옴\n",
    "#     select_id = max(sample_idx - select_id, 0)\n",
    "    \n",
    "#     # load images\n",
    "#     fname = erp_files[select_id]\n",
    "#     adj_erp_img = get_erp_img(fname)\n",
    "    \n",
    "#     adj_erp_imgs.append(adj_erp_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195c22f2-29bf-4ef0-afde-a3cc045ee7eb",
   "metadata": {},
   "source": [
    "#### vqueue\n",
    "- sample_idx\n",
    "- tangent patches\n",
    "- sensor2sensor (직전 frame 대비 pose 변화)\n",
    "- ego pose (현재 pose -> 누적해서 계산해야함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00d26fec-64d8-4b6a-8b57-57368d277218",
   "metadata": {},
   "outputs": [],
   "source": [
    "tangent_intrinsics = {'CAM_FRONT_LEFT': [[1.31669199e+03, 0.00000000e+00, 7.71567974e+02], # Tangent location 0 (leftmost)\n",
    "                                         [0.00000000e+00, 1.30594375e+03, 4.27529182e+02],\n",
    "                                         [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]],\n",
    "                      'CAM_FRONT': [[1.32277551e+03, 0.00000000e+00, 7.56801337e+02], # Tangent location 1\n",
    "                                    [0.00000000e+00, 1.31076362e+03, 4.17111552e+02],\n",
    "                                    [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]],\n",
    "                      'CAM_FRONT_RIGHT': [[1.31303854e+03, 0.00000000e+00, 7.16879740e+02], # Tangent location 2\n",
    "                                          [0.00000000e+00, 1.30008012e+03, 4.21897818e+02],\n",
    "                                          [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]],\n",
    "                      'CAM_BACK_LEFT': [[1.31538668e+03, 0.00000000e+00, 7.62655552e+02], # Tangent location 3\n",
    "                                        [0.00000000e+00, 1.30582175e+03, 4.21392564e+02],\n",
    "                                        [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]],\n",
    "                      'CAM_BACK': [[1.37645753e+03, 0.00000000e+00, 7.33078005e+02], # Tangent location 4\n",
    "                                   [0.00000000e+00, 1.36126888e+03, 4.13681559e+02],\n",
    "                                   [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]],\n",
    "                      'CAM_BACK_RIGHT': [[1.33525055e+03, 0.00000000e+00, 7.02715248e+02], # Tangent location 5\n",
    "                                         [0.00000000e+00, 1.32071237e+03, 4.12790092e+02],\n",
    "                                         [0.00000000e+00, 0.00000000e+00, 1.00000000e+00]]}\n",
    "\n",
    "# sensor2ego rotation 변경!\n",
    "sensor2ego_rot_eulers = {'CAM_FRONT_LEFT': [-90.0, 0.0, 60.0],\n",
    "                         'CAM_FRONT': [-90.0, 0.0, 0.0],\n",
    "                         'CAM_FRONT_RIGHT': [-90.0, 0.0, -60.0],\n",
    "                         'CAM_BACK_LEFT': [-90.0, 0.0, 240.0],\n",
    "                         'CAM_BACK': [-90.0, 0.0, 180.0],\n",
    "                         'CAM_BACK_RIGHT': [-90.0, 0.0, 120.0]}\n",
    "\n",
    "# x, y 좌표는 ego sensor와 virtual tangent sensor 간에 차이가 없는게 맞음\n",
    "# 그럼 왜 z는?\n",
    "# sensor2ego_trans = [0.0, 0.0, 0.0]\n",
    "sensor2ego_trans = [0.0, 0.0, 1.5]\n",
    "\n",
    "# sensor2ego matrix\n",
    "sensor2ego_mats = {}\n",
    "for cam in cfg.data_config['cams']:\n",
    "    sensor2ego_degrees = sensor2ego_rot_eulers[cam]\n",
    "    sensor2ego_radians = [degree * np.pi / 180 for degree in sensor2ego_degrees]\n",
    "    sensor2ego_q = Quaternion(get_quaternion_from_euler(sensor2ego_radians))\n",
    "    w, x, y, z = sensor2ego_q\n",
    "\n",
    "    sensor2ego_rot = torch.Tensor(Quaternion(w, x, y, z).rotation_matrix)\n",
    "    sensor2ego_tran = torch.Tensor(sensor2ego_trans)\n",
    "\n",
    "    sensor2ego = sensor2ego_rot.new_zeros((4, 4))\n",
    "    sensor2ego[3, 3] = 1\n",
    "    sensor2ego[:3, :3] = sensor2ego_rot\n",
    "    sensor2ego[:3, -1] = sensor2ego_tran\n",
    "    \n",
    "    sensor2ego_mats[cam] = sensor2ego\n",
    "\n",
    "# calibrated_sensors = []\n",
    "# for cam in cfg.data_config['cams']:\n",
    "    \n",
    "# ego2global_rotation = np.array([1.0, 0.0, 0.0, 0.0])\n",
    "# ego2global_translation = np.array([0.0, 0.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ae5b07cf-eb72-463f-bbe4-4e191ce516a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_num = cfg.multi_adj_frame_id_cfg[1]\n",
    "vqueue = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313af129-5c76-499c-af1c-435a602bb600",
   "metadata": {},
   "source": [
    "### TODO\n",
    "- 실제 환경에서 video frame 하나씩 받으면서 처리하는 것처럼 구현\n",
    "- 여러 tensor들을 언제 GPU memory에 올리고 언제 free시킬지 => memory management 구현해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7953f3c2-6a5a-43b1-bba4-c634ca8f5163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load erp image: ../data/daejeon_road_outdoor/erp_images/scene_3/frame_0051.jpg\n",
      "pose translation tensor([[[ 0.0010, -0.0001, -0.0166]]], device='cuda:0')\n",
      "pose translation tensor([[[ 6.8173e-04,  3.3688e-05, -1.4935e-02]]], device='cuda:0')\n",
      "pose translation tensor([[[ 1.5119e-03, -6.7083e-05, -1.3025e-02]]], device='cuda:0')\n",
      "pose translation tensor([[[ 0.0009,  0.0004, -0.0284]]], device='cuda:0')\n",
      "pose translation tensor([[[ 1.8333e-03, -1.1520e-05, -2.3840e-02]]], device='cuda:0')\n",
      "pose translation tensor([[[-9.6593e-04,  8.0236e-05, -1.5609e-02]]], device='cuda:0')\n",
      "tensor([[ 9.9958e-01, -6.5241e-05,  2.1651e-02,  8.2231e-04],\n",
      "        [ 1.1601e-04,  9.9998e-01,  7.1785e-04,  4.2852e-05],\n",
      "        [-2.1652e-02, -7.2448e-04,  9.9957e-01, -1.8722e-02],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]], device='cuda:0')\n",
      "tensor([[ 9.9995e-01,  4.9546e-05, -2.1659e-02,  9.9999e+01],\n",
      "        [-1.3156e-04,  1.0000e+00, -7.1532e-04,  1.0000e+02],\n",
      "        [ 2.1660e-02,  7.2588e-04,  9.9996e-01,  1.8703e-02],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 50\n",
    "cur_erp_img = get_erp_img(erp_files[sample_idx]) # current erp image\n",
    "\n",
    "# 먼저 현재 frame에서 tangent patches 만들어 내고 그 다음에 queue에 삽입\n",
    "with torch.no_grad():\n",
    "    \n",
    "    \"\"\" 1. 현재 frame (sample_idx)에 대한 정보들 계산 후 큐 저장 \"\"\"\n",
    "    \n",
    "    cur_data = dict(sample_idx=sample_idx)\n",
    "    cur_erp_img = cur_erp_img.to(device)\n",
    "    \n",
    "    # Collect tangent patches of erp image\n",
    "    persp = F.grid_sample(cur_erp_img, grid, mode='bilinear', padding_mode='zeros', align_corners=True)\n",
    "    persp_reshape = F.unfold(persp, kernel_size=(tangent_h, tangent_w), stride=(tangent_h, tangent_w))\n",
    "    persp_seq = persp_reshape.reshape(1, 3, tangent_h, tangent_w, n_patch)\n",
    "    \n",
    "    patches = list()\n",
    "    pose_inputs = list()\n",
    "    for cam_idx in range(len(cfg.data_config['cams'])):\n",
    "        patch = persp_seq[0, :, :, :, cam_idx]\n",
    "\n",
    "        # Color change\n",
    "        patch = patch.permute(1, 2, 0).cpu().numpy()\n",
    "        patch = patch * 255\n",
    "        patch = patch[:,:,[2,1,0]].astype(np.uint8)\n",
    "\n",
    "        patch = transforms.ToPILImage()(patch) # time (ms): 5.7\n",
    "        # display(patch)\n",
    "        \n",
    "        # Camera pose est. input\n",
    "        campose_input = patch.crop(campose_input_crop)\n",
    "        campose_input = transforms.ToTensor()(campose_input).unsqueeze(0)\n",
    "        campose_input = campose_input.to(device)\n",
    "        pose_inputs.append(campose_input)\n",
    "        \n",
    "        # image transformation (resize, crop, flip, rotate, ...)\n",
    "        # Resizing images is already done in tangent projection, only cropping is applied\n",
    "        patch = patch.crop(crop)\n",
    "        patch = mmcv.imnormalize(np.array(patch),\n",
    "                                 np.array(img_conf['img_mean'], np.float32), # TODO check: img_mean, img_std?\n",
    "                                 np.array(img_conf['img_std'], np.float32),\n",
    "                                 img_conf['to_rgb']) # time (ms):  5.79\n",
    "        # patch = torch.from_numpy(patch).permute(2, 0, 1) # BEVDepth 버전\n",
    "        patch = torch.tensor(patch).float().permute(2, 0, 1).contiguous() # BEVDet 버전 - 결과는 같음, 시간차이?\n",
    "        patch = patch.to(device) # time (ms): 1.20\n",
    "        patches.append(patch)\n",
    "    \n",
    "    cur_data['patches'] = patches\n",
    "    cur_data['pose_inputs'] = pose_inputs\n",
    "    \n",
    "    # Camera pose\n",
    "    if len(vqueue) > 0:\n",
    "        # Relative camera pose estimation btw. last frame and current frame\n",
    "        prev_data = vqueue[-1] # Lastest frame\n",
    "        prev_pose_inputs = prev_data['pose_inputs']\n",
    "        sweep_pose_inputs = [torch.stack(pose_inputs), torch.stack(prev_pose_inputs)]\n",
    "        \n",
    "        # Transformation matrix for previous camera (sensor) frame to current camera frame\n",
    "        sensor2sensor_mats = list()\n",
    "        for cam_idx in range(len(cfg.data_config['cams'])):\n",
    "            source_image = sweep_pose_inputs[0][cam_idx]\n",
    "            target_image = sweep_pose_inputs[1][cam_idx]\n",
    "            pose_inputs = [source_image, target_image]\n",
    "            pose_inputs = [pose_enc(torch.cat(pose_inputs, 1))]\n",
    "            axisangle, translation = pose_dec(pose_inputs)\n",
    "            # print(\"CAM\", cfg.data_config['cams'][cam_idx])\n",
    "            # print(\"pose axis angle\", axisangle)\n",
    "            print(\"pose translation\", translation[:, 0])\n",
    "            pose = transformation_from_parameters(axisangle[:, 0], translation[:, 0], invert=False)\n",
    "            # print(f\"Pose matrix for image pair {cam_idx}: \\n{pose}\")\n",
    "            sensor2sensor_mats.append(pose)\n",
    "        \n",
    "        sensor2sensor_mat_sum = sensor2sensor_mats[0].new_zeros((4, 4))\n",
    "        for mat in sensor2sensor_mats:\n",
    "            sensor2sensor_mat_sum += mat\n",
    "        prev_sensor2cur_sensor = sensor2sensor_mat_sum / len(sensor2sensor_mats) # average\n",
    "        \n",
    "        print(prev_sensor2cur_sensor)\n",
    "        \n",
    "        cur_sensor2prev_sensor = prev_sensor2cur_sensor.inverse()\n",
    "        \n",
    "        prev_ego2global = prev_data['ego2global'].to(device)\n",
    "        cur_ego2global = prev_ego2global @ cur_sensor2prev_sensor\n",
    "        \n",
    "#         # Calculate ego2global (ego pose) of the current frame\n",
    "#         ego2global_sum = torch.zeros(4, 4).to(device)\n",
    "#         prev_ego2global = prev_data['ego2global'].to(device)\n",
    "#         for cam_idx, cam in enumerate(cfg.data_config['cams']):\n",
    "#             sensor2ego_mat = sensor2ego_mats[cam].to(device)\n",
    "#             cur_sensor2prev_sensor = sensor2sensor_mats[cam_idx].inverse().to(device)\n",
    "#             ego2sensor_mat = sensor2ego_mat.inverse().to(device)\n",
    "            \n",
    "#             cam_ego2global = prev_ego2global @ sensor2ego_mat @ cur_sensor2prev_sensor @ ego2sensor_mat\n",
    "#             ego2global_sum += cam_ego2global\n",
    "#         cur_ego2global = ego2global_sum / len(cfg.data_config['cams'])\n",
    "        \n",
    "        print(cur_ego2global)\n",
    "        \n",
    "        \"\"\" \n",
    "        We have,\n",
    "        prev ego -> global\n",
    "        prev cam -> cur cam\n",
    "        cam -> ego (static)\n",
    "        \n",
    "        Calculate, \n",
    "        cur_ego2global <= prev_ego2global @ prev_cam2ego @ cur_cam2prev_cam @ cur_ego2cur_cam\n",
    "        \n",
    "        global2prev_ego @ ego2cam @ prev_cam2cur_cam @ cam2ego == global2cur_ego\n",
    "        global -> prev ego -> prev cam -> cur cam -> ego\n",
    "        \n",
    "        \"\"\" \n",
    "    \n",
    "    else: # Initial step\n",
    "        # initial ego pose matrix\n",
    "        w, x, y, z = np.array([1.0, 0.0, 0.0, 0.0])\n",
    "        ego2global_rot = torch.Tensor(Quaternion(w, x, y, z).rotation_matrix)\n",
    "        ego2global_tran = torch.Tensor(np.array([100.0, 100.0, 0.0]))\n",
    "        ego2global = ego2global_rot.new_zeros((4, 4))\n",
    "        ego2global[3, 3] = 1\n",
    "        ego2global[:3, :3] = ego2global_rot\n",
    "        ego2global[:3, -1] = ego2global_tran\n",
    "        \n",
    "        cur_data['ego2global'] = ego2global\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # 현재 frame의 sensor2keyego 및 sensor2sensor (self2self) 구하기\n",
    "    \n",
    "    \n",
    "    # cur_data 업데이트 한 뒤 vqueue에 저장\n",
    "    vqueue.append(cur_data)\n",
    "    \n",
    "    \n",
    "    \"\"\" 2. BEVDet4D input 준비 \"\"\"\n",
    "    \n",
    "#     # Prepate BEVDet4D inputs\n",
    "#     v_patches = list()\n",
    "#     for select_id in range(*cfg.multi_adj_frame_id_cfg): # Previous 8 frames\n",
    "#         select_id = max(sample_idx - select_id, 0)\n",
    "        \n",
    "#         if select_id == cur_data['sample_idx']: # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d98d47-d6e4-4b86-8878-48e457d460aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864e131d-16bd-465c-bc96-aa3b9c7d9a36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3146e45-565d-4771-89c4-00826662280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Re-shape patches and prepare camera pose estimation inputs\n",
    "    \n",
    "\n",
    "            # image transformation (resize, crop, flip, rotate, ...)\n",
    "            # Resizing images is already done in tangent projection, only cropping is applied\n",
    "            patch = patch.crop(crop)\n",
    "            # display(patch)\n",
    "            \n",
    "            patch = mmcv.imnormalize(np.array(patch), \n",
    "                                     np.array(img_conf['img_mean'], np.float32), # TODO check: img_mean, img_std?\n",
    "                                     np.array(img_conf['img_std'], np.float32),\n",
    "                                     img_conf['to_rgb']) # time (ms):  5.79\n",
    "            # patch = torch.from_numpy(patch).permute(2, 0, 1) # BEVDepth 버전\n",
    "            patch = torch.tensor(patch).float().permute(2, 0, 1).contiguous() # BEVDet 버전 - 결과는 같음, 시간차이?\n",
    "            patch = patch.to(device) # time (ms):  1.20\n",
    "        \n",
    "        sweep_pose_inputs.append(torch.stack(campose_inputs))\n",
    "    sweep_patches = torch.stack(sweep_patches, 0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a3a246-2dac-4733-ac13-3a15e7477f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_inputs = data['img_inputs'][0] # (imgs, rots, trans, intrins, post_rots, post_trans)\n",
    "sample_token = data['img_metas'][0].data[0][0]['sample_idx']\n",
    "data['img_metas'] = data['img_metas'][0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66374ae5-6d20-4b13-92b5-85895e1a4ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f673fe46-4ea7-474c-ae76-be90c2c5b8af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412f6be8-1a8f-4696-ab3b-393aa05cdfc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44782bf5-203f-4433-9892-048208f08a15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d04163-e979-401d-b43b-19a40f37008e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # nuscenes inference\n",
    "# sum = 0\n",
    "# cnt = 0\n",
    "# for j in range(0, 100):\n",
    "#     cnt += 1\n",
    "#     data = next(data_iterator)\n",
    "    \n",
    "#     sample_token = data['img_metas'][0].data[0][0]['sample_idx']\n",
    "#     data['img_metas'] = data['img_metas'][0].data\n",
    "\n",
    "#     with torch.no_grad():\n",
    "\n",
    "#         data['points'][0].data[0][0] = data['points'][0].data[0][0].to(device)\n",
    "\n",
    "#         img_inputs = data['img_inputs'][0] # (imgs, rots, trans, intrins, post_rots, post_trans)\n",
    "\n",
    "#         for i, input_d in enumerate(img_inputs):\n",
    "#             img_inputs[i] = input_d.to(device)\n",
    "#         data['img_inputs'][0] = img_inputs\n",
    "#         # data['img_inputs'][0][0] = data['img_inputs'][0][0].to(device)\n",
    "\n",
    "#         starter.record()\n",
    "\n",
    "#         outputs = model(return_loss=False, rescale=True, **data)\n",
    "\n",
    "#         ender.record()\n",
    "#         torch.cuda.synchronize()\n",
    "#         inference_time = starter.elapsed_time(ender)\n",
    "#         sum += inference_time\n",
    "#         print(\"inference time (ms): \", inference_time)\n",
    "# print(sum / cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340117fb-76f5-4735-8afe-db27f5809877",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# nuscenes inference\n",
    "sample_token = data['img_metas'][0].data[0][0]['sample_idx']\n",
    "data['img_metas'] = data['img_metas'][0].data\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    data['points'][0].data[0][0] = data['points'][0].data[0][0].to(device)\n",
    "    \n",
    "    img_inputs = data['img_inputs'][0] # (imgs, rots, trans, intrins, post_rots, post_trans)\n",
    "    \n",
    "    for i, input_d in enumerate(img_inputs):\n",
    "        img_inputs[i] = input_d.to(device)\n",
    "    data['img_inputs'][0] = img_inputs\n",
    "    # data['img_inputs'][0][0] = data['img_inputs'][0][0].to(device)\n",
    "    \n",
    "    starter.record()\n",
    "    \n",
    "    outputs = model(return_loss=False, rescale=True, **data)\n",
    "    \n",
    "    ender.record()\n",
    "    torch.cuda.synchronize()\n",
    "    inference_time = starter.elapsed_time(ender)\n",
    "    print(\"inference time (ms): \", inference_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bdc631-25ab-4b4c-96bf-7d52e00ea52e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rank, _ = get_dist_info()\n",
    "if rank == 0:\n",
    "    dataset.format_single_result(outputs, sample_idx, jsonfile_prefix='./output/')\n",
    "\n",
    "results_path = './output/pts_bbox/results_nusc.json'\n",
    "root_path = '../data/nuscenes'\n",
    "\n",
    "# load predicted results\n",
    "results = mmcv.load(results_path)['results']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df41df4-dfb8-4442-a27c-bda5ebbf03ee",
   "metadata": {},
   "source": [
    "## Nusc 기반 vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b60982d-a139-4dff-aca2-ab90db11b5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "views = [\n",
    "    'CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_BACK_LEFT',\n",
    "    'CAM_BACK', 'CAM_BACK_RIGHT'\n",
    "]\n",
    "\n",
    "info = infos[vis_idx]\n",
    "\n",
    "show_classes=[\n",
    "    'car',\n",
    "    'truck',\n",
    "    'construction_vehicle',\n",
    "    'bus',\n",
    "    'trailer',\n",
    "    'barrier',\n",
    "    'motorcycle',\n",
    "    'bicycle',\n",
    "    'pedestrian',\n",
    "    'traffic_cone',\n",
    "]\n",
    "\n",
    "# Set cameras\n",
    "threshold = 0.3\n",
    "show_range = 60\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(21, 8))\n",
    "\n",
    "imsize = (1600, 900)\n",
    "box_vis_level = BoxVisibility.ANY\n",
    "\n",
    "for i, k in enumerate(views):\n",
    "    # Draw camera views\n",
    "    fig_idx = i + 1 if i < 3 else i + 1\n",
    "    plt.subplot(2, 3, fig_idx)\n",
    "\n",
    "    # Set camera attributes\n",
    "    plt.title(k)\n",
    "    plt.axis('off')\n",
    "    plt.xlim(0, 1600)\n",
    "    plt.ylim(900, 0)\n",
    "\n",
    "    img = mmcv.imread(os.path.join(info['cams'][k]['data_path']))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Draw images\n",
    "    plt.imshow(img)\n",
    "\n",
    "    \"\"\"\n",
    "    Box: global => ego => lidar => sensor => image?\n",
    "      or global => ego => sensor => image?\n",
    "    \"\"\"\n",
    "    ego2global_translation = info['cams'][k]['ego2global_translation']\n",
    "    ego2global_rotation = info['cams'][k]['ego2global_rotation']\n",
    "    \n",
    "    lidar2ego_translation = info['lidar2ego_translation']\n",
    "    lidar2ego_rotation =  Quaternion(info['ego2global_rotation'])\n",
    "    \n",
    "    sensor2ego_trans = info['cams'][k]['sensor2ego_translation']\n",
    "    sensor2ego_rot = info['cams'][k]['sensor2ego_rotation']\n",
    "    \n",
    "    sensor2lidar_translation = info['cams'][k]['sensor2lidar_translation']\n",
    "    sensor2lidar_rotation = info['cams'][k]['sensor2lidar_rotation']\n",
    "    \n",
    "    intrinsic = info['cams'][k]['cam_intrinsic']\n",
    "    intrinsic = np.array(intrinsic)\n",
    "    \n",
    "    boxes_pred = []\n",
    "    for box_dict in list(results.values())[0]:\n",
    "        if box_dict['detection_score'] >= threshold and box_dict['detection_name'] in show_classes:\n",
    "            box = Box(\n",
    "                box_dict['translation'],\n",
    "                box_dict['size'],\n",
    "                Quaternion(box_dict['rotation']),\n",
    "                name=box_dict['detection_name']\n",
    "            )\n",
    "            \n",
    "            # box를 global => ego로 이동\n",
    "            box.translate(-np.array(ego2global_translation))\n",
    "            box.rotate(Quaternion(ego2global_rotation).inverse)\n",
    "            \n",
    "            # box를 ego => lidar로 이동\n",
    "            # box.translate(np.array(lidar2ego_translation))\n",
    "            # box.rotate(Quaternion(lidar2ego_rotation).inverse)\n",
    "            \n",
    "            # box를 lidar => camera로 이동\n",
    "            # box.translate(-np.array(sensor2lidar_translation))\n",
    "            # box.rotate(Quaternion(matrix=sensor2lidar_rotation).inverse)\n",
    "            \n",
    "            # box를 ego => camera로 이동\n",
    "            box.translate(-np.array(sensor2ego_trans))\n",
    "            box.rotate(Quaternion(sensor2ego_rot).inverse)\n",
    "            \n",
    "            if box_in_image(box, intrinsic, imsize, vis_level=box_vis_level):\n",
    "                c=cm.get_cmap('tab10')(show_classes.index(box.name))\n",
    "                \n",
    "                # box를 camera => image로 이동해서 render\n",
    "                box.render(plt, view=intrinsic, normalize=True, colors=(c, c, c))\n",
    "\n",
    "# Set legend\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(),\n",
    "           by_label.keys(),\n",
    "           loc='upper right',\n",
    "           framealpha=1)\n",
    "\n",
    "plt.tight_layout(w_pad=0, h_pad=2)\n",
    "# save_name ='output_%06d.jpg' % idx\n",
    "# plt.savefig(save_path+save_name)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6bae4d-54bf-4c99-b1e7-3dd8b3067eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnicv",
   "language": "python",
   "name": "omnicv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
